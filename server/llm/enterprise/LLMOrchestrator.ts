/**
 * LLMOrchestrator - Enterprise-grade LLM orchestration with intelligent routing and optimization
 * Handles complex workflow-aware routing, load balancing, and performance optimization
 */

import { llmRegistry, LLMProvider, LLMResult, LLMMessage, LLMModelId } from '../LLMProvider';
import { llmFallbackManager } from '../LLMFallbackManager';
import { llmAnalytics } from '../LLMAnalytics';

export interface OrchestrationContext {
  workflowId: string;
  nodeId: string;
  userId?: string;
  priority: 'low' | 'medium' | 'high' | 'critical';
  deadline?: number; // timestamp
  resourceConstraints?: {
    maxCost?: number;
    maxLatency?: number;
    preferredRegions?: string[];
  };
  dataClassification?: 'public' | 'internal' | 'confidential' | 'restricted';
  compliance?: string[]; // ['GDPR', 'HIPAA', 'SOX']
}

export interface LLMInstance {
  id: string;
  provider: string;
  model: string;
  region: string;
  endpoint: string;
  capacity: {
    current: number;
    maximum: number;
    reserved: number;
  };
  performance: {
    avgLatency: number;
    successRate: number;
    costPerToken: number;
  };
  compliance: string[];
  status: 'active' | 'degraded' | 'maintenance' | 'offline';
  lastHealthCheck: number;
}

export interface RoutingDecision {
  selectedInstance: LLMInstance;
  reasoning: string;
  confidence: number;
  fallbackInstances: LLMInstance[];
  estimatedCost: number;
  estimatedLatency: number;
}

export interface OrchestrationMetrics {
  totalRequests: number;
  routingAccuracy: number;
  avgDecisionTime: number;
  costSavings: number;
  performanceGains: number;
  complianceViolations: number;
}

class WorkflowPatternAnalyzer {
  private patterns = new Map<string, WorkflowPattern>();

  analyzeWorkflow(workflowId: string, nodeTypes: string[]): WorkflowPattern {
    const patternKey = this.generatePatternKey(nodeTypes);
    
    if (!this.patterns.has(patternKey)) {
      this.patterns.set(patternKey, {
        id: patternKey,
        nodeTypes,
        complexity: this.calculateComplexity(nodeTypes),
        estimatedExecutionTime: this.estimateExecutionTime(nodeTypes),
        resourceRequirements: this.estimateResourceRequirements(nodeTypes),
        recommendedStrategy: this.recommendStrategy(nodeTypes),
        usageCount: 0,
        successRate: 1.0
      });
    }

    const pattern = this.patterns.get(patternKey)!;
    pattern.usageCount++;
    
    return pattern;
  }

  private generatePatternKey(nodeTypes: string[]): string {
    return nodeTypes.sort().join('->');
  }

  private calculateComplexity(nodeTypes: string[]): number {
    const complexityWeights = {
      'action.llm.generate': 2,
      'action.llm.extract': 3,
      'action.llm.classify': 2,
      'action.llm.tool_call': 4,
      'default': 1
    };

    return nodeTypes.reduce((total, type) => {
      return total + (complexityWeights[type] || complexityWeights.default);
    }, 0);
  }

  private estimateExecutionTime(nodeTypes: string[]): number {
    const timeWeights = {
      'action.llm.generate': 2000,
      'action.llm.extract': 1500,
      'action.llm.classify': 1000,
      'action.llm.tool_call': 3000,
      'default': 500
    };

    return nodeTypes.reduce((total, type) => {
      return total + (timeWeights[type] || timeWeights.default);
    }, 0);
  }

  private estimateResourceRequirements(nodeTypes: string[]): ResourceRequirements {
    const llmNodeCount = nodeTypes.filter(type => type.startsWith('action.llm')).length;
    
    return {
      cpu: Math.min(100, llmNodeCount * 20),
      memory: Math.min(1000, llmNodeCount * 100),
      bandwidth: Math.min(1000, llmNodeCount * 50),
      storage: Math.min(100, llmNodeCount * 10)
    };
  }

  private recommendStrategy(nodeTypes: string[]): string {
    const llmNodes = nodeTypes.filter(type => type.startsWith('action.llm')).length;
    const hasToolCalls = nodeTypes.some(type => type.includes('tool_call'));
    
    if (hasToolCalls) return 'high-compute';
    if (llmNodes > 5) return 'distributed';
    if (llmNodes > 2) return 'batch-optimized';
    return 'standard';
  }
}

interface WorkflowPattern {
  id: string;
  nodeTypes: string[];
  complexity: number;
  estimatedExecutionTime: number;
  resourceRequirements: ResourceRequirements;
  recommendedStrategy: string;
  usageCount: number;
  successRate: number;
}

interface ResourceRequirements {
  cpu: number;
  memory: number;
  bandwidth: number;
  storage: number;
}

class IntelligentRouter {
  private instances = new Map<string, LLMInstance>();
  private routingHistory: RoutingDecision[] = [];

  registerInstance(instance: LLMInstance): void {
    this.instances.set(instance.id, instance);
    console.log(`üéØ Registered LLM instance: ${instance.id} (${instance.provider}:${instance.model})`);
  }

  async routeRequest(
    request: {
      model: LLMModelId;
      messages: LLMMessage[];
      context: OrchestrationContext;
    }
  ): Promise<RoutingDecision> {
    const startTime = Date.now();
    
    // Get available instances that support the model
    const availableInstances = this.getAvailableInstances(request.model, request.context);
    
    if (availableInstances.length === 0) {
      throw new Error(`No available instances for model ${request.model}`);
    }

    // Score instances based on multiple factors
    const scoredInstances = this.scoreInstances(availableInstances, request.context);
    
    // Select best instance
    const selectedInstance = scoredInstances[0].instance;
    const fallbackInstances = scoredInstances.slice(1, 4).map(s => s.instance);

    const decision: RoutingDecision = {
      selectedInstance,
      reasoning: this.generateReasoning(scoredInstances[0]),
      confidence: scoredInstances[0].score,
      fallbackInstances,
      estimatedCost: this.estimateCost(selectedInstance, request),
      estimatedLatency: selectedInstance.performance.avgLatency
    };

    // Record routing decision
    this.routingHistory.push(decision);
    this.trimHistory();

    const decisionTime = Date.now() - startTime;
    console.log(`üéØ Routing decision made in ${decisionTime}ms: ${selectedInstance.id}`);

    return decision;
  }

  private getAvailableInstances(model: LLMModelId, context: OrchestrationContext): LLMInstance[] {
    const modelProvider = model.split(':')[0];
    
    return Array.from(this.instances.values()).filter(instance => {
      // Check provider compatibility
      if (instance.provider !== modelProvider) return false;
      
      // Check capacity
      if (instance.capacity.current >= instance.capacity.maximum) return false;
      
      // Check status
      if (instance.status !== 'active') return false;
      
      // Check compliance requirements
      if (context.compliance && context.compliance.length > 0) {
        const hasRequiredCompliance = context.compliance.every(req => 
          instance.compliance.includes(req)
        );
        if (!hasRequiredCompliance) return false;
      }
      
      // Check data classification
      if (context.dataClassification === 'restricted') {
        // Only allow instances with strict compliance
        if (!instance.compliance.includes('SOX') && !instance.compliance.includes('HIPAA')) {
          return false;
        }
      }
      
      return true;
    });
  }

  private scoreInstances(
    instances: LLMInstance[],
    context: OrchestrationContext
  ): Array<{ instance: LLMInstance; score: number; breakdown: any }> {
    const scored = instances.map(instance => {
      const breakdown = this.calculateInstanceScore(instance, context);
      const score = Object.values(breakdown).reduce((sum, val) => sum + val, 0);
      
      return { instance, score, breakdown };
    });

    return scored.sort((a, b) => b.score - a.score);
  }

  private calculateInstanceScore(instance: LLMInstance, context: OrchestrationContext): any {
    const weights = {
      performance: 0.3,
      cost: 0.2,
      capacity: 0.2,
      compliance: 0.15,
      region: 0.1,
      reliability: 0.05
    };

    const scores = {
      performance: this.scorePerformance(instance, context),
      cost: this.scoreCost(instance, context),
      capacity: this.scoreCapacity(instance),
      compliance: this.scoreCompliance(instance, context),
      region: this.scoreRegion(instance, context),
      reliability: this.scoreReliability(instance)
    };

    // Apply weights
    const weightedScores: any = {};
    for (const [key, score] of Object.entries(scores)) {
      weightedScores[key] = score * weights[key as keyof typeof weights];
    }

    return weightedScores;
  }

  private scorePerformance(instance: LLMInstance, context: OrchestrationContext): number {
    let score = 0.5; // Base score

    // Latency scoring
    const maxAcceptableLatency = context.resourceConstraints?.maxLatency || 10000;
    const latencyScore = Math.max(0, 1 - (instance.performance.avgLatency / maxAcceptableLatency));
    score += latencyScore * 0.4;

    // Success rate scoring
    score += instance.performance.successRate * 0.4;

    // Priority boost
    if (context.priority === 'critical') {
      score *= 1.2;
    } else if (context.priority === 'high') {
      score *= 1.1;
    }

    // Deadline consideration
    if (context.deadline) {
      const timeRemaining = context.deadline - Date.now();
      if (timeRemaining < instance.performance.avgLatency * 2) {
        score *= 0.5; // Penalize if unlikely to meet deadline
      }
    }

    return Math.min(1, score);
  }

  private scoreCost(instance: LLMInstance, context: OrchestrationContext): number {
    if (!context.resourceConstraints?.maxCost) return 0.5;
    
    const estimatedCost = instance.performance.costPerToken * 1000; // Estimate for 1k tokens
    const maxCost = context.resourceConstraints.maxCost;
    
    return Math.max(0, 1 - (estimatedCost / maxCost));
  }

  private scoreCapacity(instance: LLMInstance): number {
    const utilization = instance.capacity.current / instance.capacity.maximum;
    return 1 - utilization; // Lower utilization = higher score
  }

  private scoreCompliance(instance: LLMInstance, context: OrchestrationContext): number {
    if (!context.compliance || context.compliance.length === 0) return 0.5;
    
    const hasAllCompliance = context.compliance.every(req => 
      instance.compliance.includes(req)
    );
    
    return hasAllCompliance ? 1 : 0;
  }

  private scoreRegion(instance: LLMInstance, context: OrchestrationContext): number {
    if (!context.resourceConstraints?.preferredRegions) return 0.5;
    
    const isPreferredRegion = context.resourceConstraints.preferredRegions.includes(instance.region);
    return isPreferredRegion ? 1 : 0.3;
  }

  private scoreReliability(instance: LLMInstance): number {
    const timeSinceHealthCheck = Date.now() - instance.lastHealthCheck;
    const healthScore = Math.max(0, 1 - (timeSinceHealthCheck / (5 * 60 * 1000))); // 5 min threshold
    
    return healthScore * instance.performance.successRate;
  }

  private generateReasoning(scored: { instance: LLMInstance; score: number; breakdown: any }): string {
    const { instance, breakdown } = scored;
    const topFactors = Object.entries(breakdown)
      .sort(([,a], [,b]) => (b as number) - (a as number))
      .slice(0, 3)
      .map(([factor, score]) => `${factor}: ${((score as number) * 100).toFixed(0)}%`);
    
    return `Selected ${instance.id} based on: ${topFactors.join(', ')}`;
  }

  private estimateCost(instance: LLMInstance, request: any): number {
    const estimatedTokens = JSON.stringify(request.messages).length / 4;
    return estimatedTokens * instance.performance.costPerToken;
  }

  private trimHistory(): void {
    if (this.routingHistory.length > 1000) {
      this.routingHistory = this.routingHistory.slice(-1000);
    }
  }

  getRoutingMetrics(): OrchestrationMetrics {
    const totalRequests = this.routingHistory.length;
    
    // Calculate routing accuracy (simplified)
    const successfulRoutes = this.routingHistory.filter(decision => 
      decision.confidence > 0.7
    ).length;
    
    return {
      totalRequests,
      routingAccuracy: totalRequests > 0 ? successfulRoutes / totalRequests : 1,
      avgDecisionTime: 45, // ms (simplified)
      costSavings: 15.5, // % (simplified)
      performanceGains: 22.3, // % (simplified)
      complianceViolations: 0
    };
  }
}

class LoadBalancer {
  private instanceLoads = new Map<string, number>();
  private healthChecks = new Map<string, number>();

  async distributeLoad(
    requests: Array<{
      id: string;
      targetInstance: LLMInstance;
      estimatedLoad: number;
    }>
  ): Promise<{ distributed: any[]; queued: any[]; rejected: any[] }> {
    const distributed = [];
    const queued = [];
    const rejected = [];

    for (const request of requests) {
      const currentLoad = this.instanceLoads.get(request.targetInstance.id) || 0;
      const availableCapacity = request.targetInstance.capacity.maximum - currentLoad;

      if (request.estimatedLoad <= availableCapacity) {
        // Can handle immediately
        this.instanceLoads.set(
          request.targetInstance.id, 
          currentLoad + request.estimatedLoad
        );
        distributed.push(request);
      } else if (this.canQueue(request)) {
        // Queue for later
        queued.push(request);
      } else {
        // Reject
        rejected.push(request);
      }
    }

    return { distributed, queued, rejected };
  }

  private canQueue(request: any): boolean {
    // Simple queuing logic - in production, implement sophisticated queuing
    return true;
  }

  releaseLoad(instanceId: string, load: number): void {
    const currentLoad = this.instanceLoads.get(instanceId) || 0;
    this.instanceLoads.set(instanceId, Math.max(0, currentLoad - load));
  }

  async performHealthCheck(instance: LLMInstance): Promise<boolean> {
    try {
      // Simplified health check - in production, ping actual endpoints
      const isHealthy = instance.status === 'active';
      this.healthChecks.set(instance.id, Date.now());
      
      if (!isHealthy) {
        console.warn(`‚ö†Ô∏è Health check failed for instance: ${instance.id}`);
      }
      
      return isHealthy;
    } catch (error) {
      console.error(`‚ùå Health check error for ${instance.id}:`, error);
      return false;
    }
  }
}

export class LLMOrchestrator {
  private patternAnalyzer = new WorkflowPatternAnalyzer();
  private router = new IntelligentRouter();
  private loadBalancer = new LoadBalancer();
  private requestQueue: any[] = [];

  constructor() {
    this.initializeDefaultInstances();
    this.startBackgroundTasks();
  }

  /**
   * Main orchestration entry point
   */
  async orchestrateRequest(
    request: {
      model: LLMModelId;
      messages: LLMMessage[];
      context: OrchestrationContext;
      nodeTypes?: string[];
    }
  ): Promise<{
    result: LLMResult;
    orchestrationData: {
      routingDecision: RoutingDecision;
      executionTime: number;
      actualCost: number;
    };
  }> {
    const startTime = Date.now();

    try {
      // Analyze workflow pattern if provided
      if (request.nodeTypes) {
        const pattern = this.patternAnalyzer.analyzeWorkflow(
          request.context.workflowId,
          request.nodeTypes
        );
        console.log(`üîç Analyzed workflow pattern: ${pattern.recommendedStrategy}`);
      }

      // Route the request
      const routingDecision = await this.router.routeRequest(request);
      
      // Execute with selected instance
      const result = await this.executeOnInstance(
        routingDecision.selectedInstance,
        request
      );

      const executionTime = Date.now() - startTime;
      
      // Log analytics
      llmAnalytics.logRequest({
        id: `orch_${Date.now()}_${Math.random().toString(36).slice(2)}`,
        timestamp: Date.now(),
        provider: routingDecision.selectedInstance.provider,
        model: request.model,
        userId: request.context.userId,
        workflowId: request.context.workflowId,
        nodeId: request.context.nodeId,
        promptTokens: result.usage?.promptTokens || 0,
        completionTokens: result.usage?.completionTokens || 0,
        totalTokens: result.usage?.promptTokens! + result.usage?.completionTokens! || 0,
        cost: result.usage?.costUSD || routingDecision.estimatedCost,
        latency: executionTime,
        success: true,
        requestType: 'orchestrated'
      });

      return {
        result,
        orchestrationData: {
          routingDecision,
          executionTime,
          actualCost: result.usage?.costUSD || routingDecision.estimatedCost
        }
      };

    } catch (error) {
      console.error('üö® Orchestration failed:', error);
      
      // Try fallback routing
      try {
        const fallbackResult = await this.handleFallback(request, error);
        return fallbackResult;
      } catch (fallbackError) {
        throw new Error(`Orchestration and fallback failed: ${error.message}`);
      }
    }
  }

  /**
   * Get orchestration metrics and insights
   */
  getOrchestrationMetrics(): {
    routing: OrchestrationMetrics;
    patterns: any;
    instances: any;
  } {
    return {
      routing: this.router.getRoutingMetrics(),
      patterns: {}, // TODO: Implement pattern metrics
      instances: {} // TODO: Implement instance metrics
    };
  }

  private async executeOnInstance(
    instance: LLMInstance,
    request: any
  ): Promise<LLMResult> {
    // Get the actual provider and execute
    const provider = llmRegistry.get(instance.provider);
    
    return await provider.generate({
      model: request.model,
      messages: request.messages,
      temperature: 0.7,
      maxTokens: 1024
    });
  }

  private async handleFallback(request: any, originalError: Error): Promise<any> {
    console.log('üîÑ Attempting fallback routing...');
    
    // Use existing fallback manager
    const fallbackResult = await llmFallbackManager.executeWithFallback({
      model: request.model,
      messages: request.messages
    });

    return {
      result: fallbackResult,
      orchestrationData: {
        routingDecision: {
          selectedInstance: { id: 'fallback', provider: fallbackResult.providerUsed },
          reasoning: 'Fallback due to primary routing failure',
          confidence: 0.5,
          fallbackInstances: [],
          estimatedCost: 0,
          estimatedLatency: fallbackResult.totalLatency
        },
        executionTime: fallbackResult.totalLatency,
        actualCost: fallbackResult.usage?.costUSD || 0
      }
    };
  }

  private initializeDefaultInstances(): void {
    // Register default LLM instances
    const defaultInstances: LLMInstance[] = [
      {
        id: 'openai-us-east-1',
        provider: 'openai',
        model: 'gpt-4o-mini',
        region: 'us-east-1',
        endpoint: 'https://api.openai.com/v1',
        capacity: { current: 0, maximum: 100, reserved: 10 },
        performance: { avgLatency: 1200, successRate: 0.98, costPerToken: 0.0001 },
        compliance: ['SOC2', 'GDPR'],
        status: 'active',
        lastHealthCheck: Date.now()
      },
      {
        id: 'openai-eu-west-1',
        provider: 'openai',
        model: 'gpt-4o-mini',
        region: 'eu-west-1',
        endpoint: 'https://api.openai.com/v1',
        capacity: { current: 0, maximum: 80, reserved: 5 },
        performance: { avgLatency: 1400, successRate: 0.97, costPerToken: 0.0001 },
        compliance: ['SOC2', 'GDPR', 'HIPAA'],
        status: 'active',
        lastHealthCheck: Date.now()
      }
    ];

    defaultInstances.forEach(instance => {
      this.router.registerInstance(instance);
    });

    console.log(`üéØ Initialized ${defaultInstances.length} default LLM instances`);
  }

  private startBackgroundTasks(): void {
    // Health check every 5 minutes
    setInterval(() => {
      this.performHealthChecks();
    }, 5 * 60 * 1000);

    // Process queued requests every 30 seconds
    setInterval(() => {
      this.processQueue();
    }, 30 * 1000);

    console.log('üîÑ Started background orchestration tasks');
  }

  private async performHealthChecks(): void {
    // TODO: Implement health checks for all instances
    console.log('üíì Performing health checks...');
  }

  private async processQueue(): void {
    if (this.requestQueue.length > 0) {
      console.log(`üìã Processing ${this.requestQueue.length} queued requests`);
      // TODO: Implement queue processing
    }
  }
}

export const llmOrchestrator = new LLMOrchestrator();